{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1hCq-x7QQTX366TboJ0YwE0Nl1LPExKae","authorship_tag":"ABX9TyPqS0Jgp1vS9fAsY80S0c/y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Crawling"],"metadata":{"id":"YmHlJdfp9dqo"}},{"cell_type":"markdown","source":["Dara Crawling atau penyerapan data adalah proses pengambilan data yang tersedia secara online untuk umum. Proses ini kemudian mengimpor informasi atau data yang telah ditemukan ke dalam file lokal di komputer Anda."],"metadata":{"id":"0BZ_3klr9ivE"}},{"cell_type":"markdown","source":["## Scrapy"],"metadata":{"id":"G-Olq5A-9qjE"}},{"cell_type":"markdown","source":["Kerangka kerja aplikasi untuk crawling web site dan mengekstraksi data terstruktur yang dapat digunakan untuk berbagai aplikasi yang bermanfaat, seperti data mining, pemrosesan informasi atau arsip sejarah."],"metadata":{"id":"pvUDkUi7-J4Z"}},{"cell_type":"markdown","source":["## Install Scrapy"],"metadata":{"id":"-XaCa-2h-OJn"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qImjuzCj9TIn","executionInfo":{"status":"ok","timestamp":1666121445235,"user_tz":-420,"elapsed":3850,"user":{"displayName":"Auliyaul Umam","userId":"14064204994466224961"}},"outputId":"4dd73acf-923e-441f-de7c-387c1f6de7f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scrapy in /usr/local/lib/python3.7/dist-packages (2.7.0)\n","Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.7/dist-packages (from scrapy) (38.0.1)\n","Requirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.9.1)\n","Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (22.1.0)\n","Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (21.1.0)\n","Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (5.5.0)\n","Requirement already satisfied: Twisted>=18.9.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (22.8.0)\n","Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.6.2)\n","Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.7/dist-packages (from scrapy) (0.2.1)\n","Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (2.0.1)\n","Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.1.0)\n","Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.6.0)\n","Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.7/dist-packages (from scrapy) (2.0.6)\n","Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (0.7.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scrapy) (21.3)\n","Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from scrapy) (57.4.0)\n","Requirement already satisfied: tldextract in /usr/local/lib/python3.7/dist-packages (from scrapy) (3.4.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.3->scrapy) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.3->scrapy) (2.21)\n","Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.7/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n","Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (22.1.0)\n","Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n","Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n","Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (15.1.0)\n","Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (20.2.0)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (4.1.1)\n","Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=18.9.0->scrapy) (2.10)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->scrapy) (3.0.9)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (3.8.0)\n","Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (1.5.1)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.24.3)\n"]}],"source":["!pip install scrapy"]},{"cell_type":"markdown","source":["## Membuat file Project Scrapy"],"metadata":{"id":"oF4hMyxO-4YE"}},{"cell_type":"code","source":["!scrapy startproject scrapyPTA"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BqxGixFc-xmt","executionInfo":{"status":"ok","timestamp":1666121474169,"user_tz":-420,"elapsed":1717,"user":{"displayName":"Auliyaul Umam","userId":"14064204994466224961"}},"outputId":"9b89a57d-d07c-4df9-8f6a-4b2972e2d29e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["New Scrapy project 'scrapyPTA', using template directory '/usr/local/lib/python3.7/dist-packages/scrapy/templates/project', created in:\n","    /content/drive/MyDrive/jupyter-book/mining/scrapyPTA\n","\n","You can start your first spider with:\n","    cd scrapyPTA\n","    scrapy genspider example example.com\n"]}]},{"cell_type":"markdown","source":["## Masuk ke Folder Project yang dibuat"],"metadata":{"id":"Ue3xhR2N_AN5"}},{"cell_type":"code","source":["%cd scrapyPTA/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k2kYwchV_Afp","executionInfo":{"status":"ok","timestamp":1666121480517,"user_tz":-420,"elapsed":349,"user":{"displayName":"Auliyaul Umam","userId":"14064204994466224961"}},"outputId":"a153c2f4-1334-4cab-d702-1a12913babda"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/jupyter-book/mining/scrapyPTA\n"]}]},{"cell_type":"markdown","source":["## Membuat File Spider\n","Membuat File Spider dan memasukkan url yang akan diambil datanya"],"metadata":{"id":"vvonV9v__Ays"}},{"cell_type":"code","source":["!scrapy genspider webpta pta.trunojoyo.ac.id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"itbRbgok_A9R","executionInfo":{"status":"ok","timestamp":1666121486266,"user_tz":-420,"elapsed":2345,"user":{"displayName":"Auliyaul Umam","userId":"14064204994466224961"}},"outputId":"9701804e-8711-47a8-e08a-038c0bb93dff"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Created spider 'webpta' using template 'basic' in module:\n","  scrapyPTA.spiders.webpta\n"]}]},{"cell_type":"markdown","source":["## Menuliskan Program Scapper"],"metadata":{"id":"AeFZ-msmAo57"}},{"cell_type":"code","source":["import scrapy\n","\n","class WebptaSpider(scrapy.Spider):\n","    name = 'webpta'\n","    allowed_domains = ['pta.trunojoyo.ac.id']\n","    start_urls = ['https://pta.trunojoyo.ac.id/c_search/byprod/12/'+str(x)+\" \" for x in range(2,20)]\n","\n","    def parse(self, response):\n","        for detail in response.css('a.gray.button::attr(href)'): \n","            yield response.follow(detail.get(), callback = self.parse_detail)\n","\n","    def parse_detail(self, response):\n","        for data in response.css('#content_journal > ul > li'):\n","            yield{\n","                'Abstraksi': data.css('div:nth-child(2) > p::text').get().replace('\\n\\n|\\n','').replace('ABSTRAK', '')\n","            }"],"metadata":{"id":"KFhaBNxxAxr7","executionInfo":{"status":"ok","timestamp":1666121491172,"user_tz":-420,"elapsed":1009,"user":{"displayName":"Auliyaul Umam","userId":"14064204994466224961"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Jalankan File Spider\n","Untuk Menjalankan file spider anda harus masuk ke directory file tersebut terlebih dahulu"],"metadata":{"id":"TzL2dIsNBBL3"}},{"cell_type":"code","source":["%cd scrapyPTA/spiders/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99QSTPBTBBUz","executionInfo":{"status":"ok","timestamp":1666121496601,"user_tz":-420,"elapsed":337,"user":{"displayName":"Auliyaul Umam","userId":"14064204994466224961"}},"outputId":"46a5642d-3756-4d55-9a06-4d10a9182276"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/jupyter-book/mining/scrapyPTA/scrapyPTA/spiders\n"]}]},{"cell_type":"code","source":["!scrapy runspider webpta.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQ8RzOMLBRtj","executionInfo":{"status":"ok","timestamp":1666121503333,"user_tz":-420,"elapsed":4715,"user":{"displayName":"Auliyaul Umam","userId":"14064204994466224961"}},"outputId":"f5dca427-8e41-4aaa-d9d6-87b60e682ad9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-10-18 19:31:39 [scrapy.utils.log] INFO: Scrapy 2.7.0 started (bot: scrapyPTA)\n","2022-10-18 19:31:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.8.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.1, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n","2022-10-18 19:31:39 [scrapy.crawler] INFO: Overridden settings:\n","{'BOT_NAME': 'scrapyPTA',\n"," 'NEWSPIDER_MODULE': 'scrapyPTA.spiders',\n"," 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n"," 'ROBOTSTXT_OBEY': True,\n"," 'SPIDER_LOADER_WARN_ONLY': True,\n"," 'SPIDER_MODULES': ['scrapyPTA.spiders'],\n"," 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n","2022-10-18 19:31:40 [asyncio] DEBUG: Using selector: EpollSelector\n","2022-10-18 19:31:40 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n","2022-10-18 19:31:40 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n","2022-10-18 19:31:40 [scrapy.extensions.telnet] INFO: Telnet Password: 93f07aaff7b9560d\n","2022-10-18 19:31:40 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","2022-10-18 19:31:40 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2022-10-18 19:31:40 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2022-10-18 19:31:40 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2022-10-18 19:31:40 [scrapy.core.engine] INFO: Spider opened\n","2022-10-18 19:31:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2022-10-18 19:31:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2022-10-18 19:31:41 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://pta.trunojoyo.ac.id/robots.txt> from <GET http://pta.trunojoyo.ac.id/robots.txt>\n","/usr/local/lib/python3.7/dist-packages/scrapy/core/engine.py:279: ScrapyDeprecationWarning: Passing a 'spider' argument to ExecutionEngine.download is deprecated\n","  return self.download(result, spider) if isinstance(result, Request) else result\n","2022-10-18 19:31:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pta.trunojoyo.ac.id/robots.txt> (referer: None)\n","2022-10-18 19:31:42 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://pta.trunojoyo.ac.id/> from <GET http://pta.trunojoyo.ac.id/>\n","2022-10-18 19:31:43 [filelock] DEBUG: Attempting to acquire lock 140645984953232 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-10-18 19:31:43 [filelock] DEBUG: Lock 140645984953232 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-10-18 19:31:43 [filelock] DEBUG: Attempting to acquire lock 140645984955216 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n","2022-10-18 19:31:43 [filelock] DEBUG: Lock 140645984955216 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n","2022-10-18 19:31:43 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): publicsuffix.org:443\n","2022-10-18 19:31:43 [urllib3.connectionpool] DEBUG: https://publicsuffix.org:443 \"GET /list/public_suffix_list.dat HTTP/1.1\" 200 None\n","2022-10-18 19:31:43 [filelock] DEBUG: Attempting to release lock 140645984955216 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n","2022-10-18 19:31:43 [filelock] DEBUG: Lock 140645984955216 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n","2022-10-18 19:31:43 [filelock] DEBUG: Attempting to release lock 140645984953232 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-10-18 19:31:43 [filelock] DEBUG: Lock 140645984953232 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-10-18 19:31:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pta.trunojoyo.ac.id/> (referer: None)\n","2022-10-18 19:31:43 [scrapy.core.engine] INFO: Closing spider (finished)\n","2022-10-18 19:31:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 896,\n"," 'downloader/request_count': 4,\n"," 'downloader/request_method_count/GET': 4,\n"," 'downloader/response_bytes': 7306,\n"," 'downloader/response_count': 4,\n"," 'downloader/response_status_count/200': 2,\n"," 'downloader/response_status_count/301': 2,\n"," 'elapsed_time_seconds': 3.14634,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2022, 10, 18, 19, 31, 43, 397869),\n"," 'httpcompression/response_bytes': 17488,\n"," 'httpcompression/response_count': 1,\n"," 'log_count/DEBUG': 17,\n"," 'log_count/INFO': 10,\n"," 'memusage/max': 140902400,\n"," 'memusage/startup': 140902400,\n"," 'response_received_count': 2,\n"," 'robotstxt/request_count': 1,\n"," 'robotstxt/response_count': 1,\n"," 'robotstxt/response_status_count/200': 1,\n"," 'scheduler/dequeued': 2,\n"," 'scheduler/dequeued/memory': 2,\n"," 'scheduler/enqueued': 2,\n"," 'scheduler/enqueued/memory': 2,\n"," 'start_time': datetime.datetime(2022, 10, 18, 19, 31, 40, 251529)}\n","2022-10-18 19:31:43 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}]},{"cell_type":"markdown","source":["## Menyimpan data abstrak ke format CSV"],"metadata":{"id":"jOWRDqwwBBdO"}},{"cell_type":"code","source":["!scrapy crawl webpta -O crawlAbstrak.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sBKty4lgBBl3","executionInfo":{"status":"ok","timestamp":1666121511506,"user_tz":-420,"elapsed":4360,"user":{"displayName":"Auliyaul Umam","userId":"14064204994466224961"}},"outputId":"93f0bac1-50ce-49f0-e8f6-2704b5ef6d13"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-10-18 19:31:48 [scrapy.utils.log] INFO: Scrapy 2.7.0 started (bot: scrapyPTA)\n","2022-10-18 19:31:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.8.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.1, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n","2022-10-18 19:31:48 [scrapy.crawler] INFO: Overridden settings:\n","{'BOT_NAME': 'scrapyPTA',\n"," 'NEWSPIDER_MODULE': 'scrapyPTA.spiders',\n"," 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n"," 'ROBOTSTXT_OBEY': True,\n"," 'SPIDER_MODULES': ['scrapyPTA.spiders'],\n"," 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n","2022-10-18 19:31:48 [asyncio] DEBUG: Using selector: EpollSelector\n","2022-10-18 19:31:48 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n","2022-10-18 19:31:48 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n","2022-10-18 19:31:48 [scrapy.extensions.telnet] INFO: Telnet Password: ace9f6ad48c9db2b\n","2022-10-18 19:31:48 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.feedexport.FeedExporter',\n"," 'scrapy.extensions.logstats.LogStats']\n","2022-10-18 19:31:48 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2022-10-18 19:31:48 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2022-10-18 19:31:48 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2022-10-18 19:31:48 [scrapy.core.engine] INFO: Spider opened\n","2022-10-18 19:31:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2022-10-18 19:31:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2022-10-18 19:31:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://pta.trunojoyo.ac.id/robots.txt> from <GET http://pta.trunojoyo.ac.id/robots.txt>\n","/usr/local/lib/python3.7/dist-packages/scrapy/core/engine.py:279: ScrapyDeprecationWarning: Passing a 'spider' argument to ExecutionEngine.download is deprecated\n","  return self.download(result, spider) if isinstance(result, Request) else result\n","2022-10-18 19:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pta.trunojoyo.ac.id/robots.txt> (referer: None)\n","2022-10-18 19:31:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://pta.trunojoyo.ac.id/> from <GET http://pta.trunojoyo.ac.id/>\n","2022-10-18 19:31:51 [filelock] DEBUG: Attempting to acquire lock 139652048729680 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-10-18 19:31:51 [filelock] DEBUG: Lock 139652048729680 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-10-18 19:31:51 [filelock] DEBUG: Attempting to release lock 139652048729680 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-10-18 19:31:51 [filelock] DEBUG: Lock 139652048729680 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-10-18 19:31:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pta.trunojoyo.ac.id/> (referer: None)\n","2022-10-18 19:31:51 [scrapy.core.engine] INFO: Closing spider (finished)\n","2022-10-18 19:31:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 896,\n"," 'downloader/request_count': 4,\n"," 'downloader/request_method_count/GET': 4,\n"," 'downloader/response_bytes': 7301,\n"," 'downloader/response_count': 4,\n"," 'downloader/response_status_count/200': 2,\n"," 'downloader/response_status_count/301': 2,\n"," 'elapsed_time_seconds': 2.779105,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2022, 10, 18, 19, 31, 51, 689605),\n"," 'httpcompression/response_bytes': 17488,\n"," 'httpcompression/response_count': 1,\n"," 'log_count/DEBUG': 11,\n"," 'log_count/INFO': 10,\n"," 'memusage/max': 141078528,\n"," 'memusage/startup': 141078528,\n"," 'response_received_count': 2,\n"," 'robotstxt/request_count': 1,\n"," 'robotstxt/response_count': 1,\n"," 'robotstxt/response_status_count/200': 1,\n"," 'scheduler/dequeued': 2,\n"," 'scheduler/dequeued/memory': 2,\n"," 'scheduler/enqueued': 2,\n"," 'scheduler/enqueued/memory': 2,\n"," 'start_time': datetime.datetime(2022, 10, 18, 19, 31, 48, 910500)}\n","2022-10-18 19:31:51 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"itoO4SMzlJ9B"},"execution_count":null,"outputs":[]}]}